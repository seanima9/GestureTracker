{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.helper import lable_dict, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using CPU\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"TensorFlow is using GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow is using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "TRAINING_DATA_DIR = \"../data/split/train\"\n",
    "VALIDATION_DATA_DIR = \"../data/split/val\"\n",
    "TEST_DATA_DIR = \"../data/split/test\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "480\n",
      "136\n"
     ]
    }
   ],
   "source": [
    "def get_data_size(data):\n",
    "    data_size = 0\n",
    "    for dir in os.listdir(data):\n",
    "        for _ in os.listdir(os.path.join(data, dir)):\n",
    "            data_size += 1\n",
    "    return data_size\n",
    "\n",
    "print(get_data_size(TRAINING_DATA_DIR))\n",
    "print(get_data_size(VALIDATION_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir):\n",
    "    \"\"\" \n",
    "    Generator function that yields data and labels from the given data directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory containing the data.\n",
    "    \n",
    "    Yields:\n",
    "        tuple: A tuple containing the data and the corresponding label. (data, label)\n",
    "        data (list): A data sample.\n",
    "        label (int): The label corresponding to the data sample.\n",
    "    \"\"\"\n",
    "    subdirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    label_to_index, _ = lable_dict(data_dir)\n",
    "    for subdir in subdirs:\n",
    "        data_files = [os.path.join(subdir, f) for f in os.listdir(subdir) if f.endswith('.json')]\n",
    "        \n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            data = [value for coordinate_dict in data for value in coordinate_dict.values()] \n",
    "            data = scaler.transform(np.array(data).reshape(-1, 1))\n",
    "            data = data.flatten().tolist()\n",
    "            label_index = label_to_index[os.path.basename(os.path.dirname(data_file))]\n",
    "            yield (data, tf.constant(label_index, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TRAINING_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(VALIDATION_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TEST_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(63,)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0011),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,\n",
    "                                                        restore_best_weights=True, min_delta= 0.01)\n",
    "\n",
    "        model.fit(training_dataset.repeat(),\n",
    "                steps_per_epoch=int(5* (get_data_size(TRAINING_DATA_DIR) / BATCH_SIZE)),\n",
    "                validation_data=validation_dataset.repeat(),\n",
    "                validation_steps=int(5* (get_data_size(VALIDATION_DATA_DIR) / BATCH_SIZE)),\n",
    "                epochs=100,\n",
    "                callbacks=[early_stopping]\n",
    "        ) \n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.6686 - loss: 0.5905 - val_accuracy: 0.7816 - val_loss: 0.4950\n",
      "Epoch 2/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7840 - loss: 0.4671 - val_accuracy: 0.7924 - val_loss: 0.4622\n",
      "Epoch 3/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7940 - loss: 0.4581 - val_accuracy: 0.7970 - val_loss: 0.4613\n",
      "Epoch 4/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8123 - loss: 0.4215 - val_accuracy: 0.7833 - val_loss: 0.5000\n",
      "Epoch 5/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8454 - loss: 0.3806 - val_accuracy: 0.8424 - val_loss: 0.3595\n",
      "Epoch 6/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.8525 - loss: 0.3697 - val_accuracy: 0.7182 - val_loss: 0.8035\n",
      "Epoch 7/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8515 - loss: 0.3708 - val_accuracy: 0.8424 - val_loss: 0.3333\n",
      "Epoch 8/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8613 - loss: 0.3273 - val_accuracy: 0.8630 - val_loss: 0.4067\n",
      "Epoch 9/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8590 - loss: 0.3305 - val_accuracy: 0.8848 - val_loss: 0.3116\n",
      "Epoch 10/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8692 - loss: 0.3180 - val_accuracy: 0.8439 - val_loss: 0.3941\n",
      "Epoch 11/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8723 - loss: 0.3232 - val_accuracy: 0.8803 - val_loss: 0.3723\n",
      "Epoch 12/100\n",
      "\u001b[1m480/480\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8881 - loss: 0.2915 - val_accuracy: 0.8727 - val_loss: 0.3170\n",
      "\u001b[1m28/28\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8583 - loss: 0.3687\n",
      "Test accuracy: 0.8823529481887817\n",
      "Test loss: 0.30607637763023376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "model = train_model(build_model())\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_dataset)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = input(\"Do you want to save the model? (y/n): \")\n",
    "\n",
    "if choice.lower() == 'y':\n",
    "    model.save(f\"../models/model_acc_{accuracy:.2f}_loss_{loss:.2f}.h5\")\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
