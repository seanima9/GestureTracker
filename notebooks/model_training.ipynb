{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.helper import label_dict, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using CPU\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"TensorFlow is using GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow is using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "TRAINING_DATA_DIR = \"../data/split/train\"\n",
    "VALIDATION_DATA_DIR = \"../data/split/val\"\n",
    "TEST_DATA_DIR = \"../data/split/test\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "856\n",
      "267\n"
     ]
    }
   ],
   "source": [
    "def get_data_size(data):\n",
    "    data_size = 0\n",
    "    for dir in os.listdir(data):\n",
    "        for _ in os.listdir(os.path.join(data, dir)):\n",
    "            data_size += 1\n",
    "    return data_size\n",
    "\n",
    "print(get_data_size(TRAINING_DATA_DIR))\n",
    "print(get_data_size(VALIDATION_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir):\n",
    "    \"\"\" \n",
    "    Generator function that yields data and labels from the given data directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory containing the data.\n",
    "    \n",
    "    Yields:\n",
    "        tuple: A tuple containing the data and the corresponding label. (data, label)\n",
    "        data (list): A data sample.\n",
    "        label (int): The label corresponding to the data sample.\n",
    "    \"\"\"\n",
    "    subdirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    label_to_index, _ = label_dict(data_dir)\n",
    "    for subdir in subdirs:\n",
    "        data_files = [os.path.join(subdir, f) for f in os.listdir(subdir) if f.endswith('.json')]\n",
    "        \n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            data = [value for coordinate_dict in data for value in coordinate_dict.values()] \n",
    "            data = scaler.transform(np.array(data).reshape(-1, 1))\n",
    "            data = data.flatten().tolist()\n",
    "            label_index = label_to_index[os.path.basename(os.path.dirname(data_file))]\n",
    "            yield (data, tf.constant(label_index, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TRAINING_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(VALIDATION_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TEST_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(63,)),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.0011),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3,\n",
    "                                                        restore_best_weights=True, min_delta= 0.01)\n",
    "\n",
    "        model.fit(training_dataset.repeat(),\n",
    "                steps_per_epoch=int(5* (get_data_size(TRAINING_DATA_DIR) / BATCH_SIZE)),\n",
    "                validation_data=validation_dataset.repeat(),\n",
    "                validation_steps=int(5* (get_data_size(VALIDATION_DATA_DIR) / BATCH_SIZE)),\n",
    "                epochs=100,\n",
    "                callbacks=[early_stopping]\n",
    "        ) \n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6151 - loss: 0.6519 - val_accuracy: 0.6221 - val_loss: 0.6161\n",
      "Epoch 2/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7407 - loss: 0.5200 - val_accuracy: 0.8167 - val_loss: 0.4029\n",
      "Epoch 3/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7710 - loss: 0.4802 - val_accuracy: 0.7265 - val_loss: 0.5162\n",
      "Epoch 4/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7825 - loss: 0.4534 - val_accuracy: 0.8424 - val_loss: 0.3798\n",
      "Epoch 5/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7949 - loss: 0.4321 - val_accuracy: 0.8311 - val_loss: 0.3980\n",
      "Epoch 6/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8131 - loss: 0.4081 - val_accuracy: 0.8341 - val_loss: 0.4134\n",
      "Epoch 7/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8359 - loss: 0.3829 - val_accuracy: 0.8371 - val_loss: 0.3570\n",
      "Epoch 8/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8226 - loss: 0.3792 - val_accuracy: 0.8258 - val_loss: 0.3939\n",
      "Epoch 9/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8283 - loss: 0.3765 - val_accuracy: 0.8598 - val_loss: 0.3353\n",
      "Epoch 10/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8442 - loss: 0.3556 - val_accuracy: 0.8167 - val_loss: 0.3859\n",
      "Epoch 11/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8374 - loss: 0.3655 - val_accuracy: 0.8295 - val_loss: 0.3556\n",
      "Epoch 12/100\n",
      "\u001b[1m856/856\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.8576 - loss: 0.3381 - val_accuracy: 0.8598 - val_loss: 0.3922\n",
      "\u001b[1m54/54\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.8601 - loss: 0.3445\n",
      "Test accuracy: 0.8576778769493103\n",
      "Test loss: 0.33236804604530334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "model = train_model(build_model())\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_dataset)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "choice = input(\"Do you want to save the model? (y/n): \")\n",
    "\n",
    "if choice.lower() == 'y':\n",
    "    model.save(f\"../models/model_acc_{accuracy:.2f}_loss_{loss:.2f}.h5\")\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
