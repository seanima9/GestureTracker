{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"TensorFlow is using GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow is using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA_DIR = \"../data/split/train\"\n",
    "VALIDATION_DATA_DIR = \"../data/split/val\"\n",
    "TEST_DATA_DIR = \"../data/split/test\"\n",
    "\n",
    "BATCH_SIZE = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_data_size():\n",
    "    data_size = 0\n",
    "    for dir in os.listdir(TRAINING_DATA_DIR):\n",
    "        for _ in os.listdir(os.path.join(TRAINING_DATA_DIR, dir)):\n",
    "            data_size += 1\n",
    "    return data_size\n",
    "\n",
    "get_training_data_size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir, batch_size):\n",
    "    \"\"\" \n",
    "    Generator function that yields a batch of data and labels from the given data directory.\n",
    "    Data file order is shuffled for each epoch.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory containing the data.\n",
    "        batch_size (int): The size of the batch.\n",
    "    \n",
    "    Yields:\n",
    "        tuple: A tuple containing the batch of data and the corresponding labels. (batch_data, batch_labels)\n",
    "        batch_data (list): A list of data samples.\n",
    "        batch_labels (list): A list of labels corresponding to the data samples.\n",
    "    \"\"\"\n",
    "    subdirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    label_to_index = {os.path.basename(label): i for i, label in enumerate(subdirs)}  # map label to index\n",
    "\n",
    "    while True:\n",
    "        np.random.shuffle(subdirs)\n",
    "        data_files = []\n",
    "        for subdir in subdirs:\n",
    "            data_files.extend([os.path.join(subdir, f) for f in os.listdir(subdir) if f.endswith('.json')])\n",
    "\n",
    "        np.random.shuffle(data_files)\n",
    "\n",
    "        for i in range(0, len(data_files), batch_size):  # len(data_files) % batch_size\n",
    "            batch_files = data_files[i:i+batch_size]\n",
    "            if len(batch_files) < batch_size:\n",
    "                continue \n",
    "            \n",
    "            batch_data = []\n",
    "            batch_labels = []\n",
    "            for data_file in batch_files:\n",
    "                with open(data_file, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "\n",
    "                data = [value for coordinate_dict in data for value in coordinate_dict.values()]  # flatten list of dictionaries\n",
    "                data = MinMaxScaler().fit_transform(np.array(data).reshape(-1, 1))\n",
    "                data = data.flatten().tolist()\n",
    "                batch_data.append(data)\n",
    "                label_index = label_to_index[os.path.basename(os.path.dirname(data_file))]\n",
    "                batch_labels.append(tf.constant(label_index, dtype=tf.int32))\n",
    "            yield (batch_data, batch_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TRAINING_DATA_DIR, BATCH_SIZE),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE, 63), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(VALIDATION_DATA_DIR, BATCH_SIZE),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE, 63), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE,), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TEST_DATA_DIR, BATCH_SIZE),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE, 63), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(BATCH_SIZE,), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, labels in test_dataset.take(1):\n",
    "    print(data)\n",
    "    print(labels)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(63,)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(16, activation='relu'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_per_epoch = get_training_data_size() // BATCH_SIZE\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "model.fit(training_dataset,\n",
    "        validation_data=validation_dataset,\n",
    "        epochs=30,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        callbacks=[early_stopping]\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
