{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import sys\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "\n",
    "from src.helper import lable_dict, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow is using CPU\n"
     ]
    }
   ],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print(\"TensorFlow is using GPU\")\n",
    "else:\n",
    "    print(\"TensorFlow is using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MinMaxScaler()\n"
     ]
    }
   ],
   "source": [
    "PROCESSED_DATA_DIR = \"../data/processed\"\n",
    "TRAINING_DATA_DIR = \"../data/split/train\"\n",
    "VALIDATION_DATA_DIR = \"../data/split/val\"\n",
    "TEST_DATA_DIR = \"../data/split/test\"\n",
    "\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "print(scaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "56\n"
     ]
    }
   ],
   "source": [
    "def get_data_size(data):\n",
    "    data_size = 0\n",
    "    for dir in os.listdir(data):\n",
    "        for _ in os.listdir(os.path.join(data, dir)):\n",
    "            data_size += 1\n",
    "    return data_size\n",
    "\n",
    "print(get_data_size(TRAINING_DATA_DIR))\n",
    "print(get_data_size(VALIDATION_DATA_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data_dir):\n",
    "    \"\"\" \n",
    "    Generator function that yields data and labels from the given data directory.\n",
    "\n",
    "    Args:\n",
    "        data_dir (str): The directory containing the data.\n",
    "    \n",
    "    Yields:\n",
    "        tuple: A tuple containing the data and the corresponding label. (data, label)\n",
    "        data (list): A data sample.\n",
    "        label (int): The label corresponding to the data sample.\n",
    "    \"\"\"\n",
    "    subdirs = [os.path.join(data_dir, d) for d in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, d))]\n",
    "    label_to_index, _ = lable_dict(data_dir)\n",
    "    for subdir in subdirs:\n",
    "        data_files = [os.path.join(subdir, f) for f in os.listdir(subdir) if f.endswith('.json')]\n",
    "        \n",
    "        for data_file in data_files:\n",
    "            with open(data_file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "            data = [value for coordinate_dict in data for value in coordinate_dict.values()] \n",
    "            data = scaler.transform(np.array(data).reshape(-1, 1))\n",
    "            data = data.flatten().tolist()\n",
    "            label_index = label_to_index[os.path.basename(os.path.dirname(data_file))]\n",
    "            yield (data, tf.constant(label_index, dtype=tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TRAINING_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "validation_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(VALIDATION_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_generator(\n",
    "    lambda: data_generator(TEST_DATA_DIR),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(63,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.int32)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_dataset = training_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "validation_dataset = validation_dataset.shuffle(1000).batch(BATCH_SIZE)\n",
    "test_dataset = test_dataset.shuffle(1000).batch(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(63,)),\n",
    "        tf.keras.layers.Dense(128, activation='relu'),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(32, activation='relu'),\n",
    "        tf.keras.layers.Dense(16, activation='relu'),\n",
    "        tf.keras.layers.Dense(2, activation='softmax')\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5,\n",
    "                                                        restore_best_weights=True, min_delta= 0.01)\n",
    "\n",
    "        model.fit(training_dataset.repeat(),\n",
    "                steps_per_epoch=int(5* (get_data_size(TRAINING_DATA_DIR) / BATCH_SIZE)),\n",
    "                validation_data=validation_dataset.repeat(),\n",
    "                validation_steps=int(5* (get_data_size(VALIDATION_DATA_DIR) / BATCH_SIZE)),\n",
    "                epochs=100,\n",
    "                callbacks=[early_stopping]\n",
    "        ) \n",
    "\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7014 - loss: 0.6125 - val_accuracy: 0.7311 - val_loss: 0.5182\n",
      "Epoch 2/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7847 - loss: 0.4960 - val_accuracy: 0.8077 - val_loss: 0.4374\n",
      "Epoch 3/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7585 - loss: 0.4818 - val_accuracy: 0.8385 - val_loss: 0.3997\n",
      "Epoch 4/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8216 - loss: 0.4230 - val_accuracy: 0.8598 - val_loss: 0.3397\n",
      "Epoch 5/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8583 - loss: 0.3352 - val_accuracy: 0.9346 - val_loss: 0.2871\n",
      "Epoch 6/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8722 - loss: 0.3010 - val_accuracy: 0.7615 - val_loss: 0.6053\n",
      "Epoch 7/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8758 - loss: 0.3205 - val_accuracy: 0.9129 - val_loss: 0.2840\n",
      "Epoch 8/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8874 - loss: 0.2667 - val_accuracy: 0.8654 - val_loss: 0.3321\n",
      "Epoch 9/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9024 - loss: 0.2696 - val_accuracy: 0.9077 - val_loss: 0.2310\n",
      "Epoch 10/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9017 - loss: 0.2411 - val_accuracy: 0.8939 - val_loss: 0.2698\n",
      "Epoch 11/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8887 - loss: 0.2441 - val_accuracy: 0.8769 - val_loss: 0.3063\n",
      "Epoch 12/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8934 - loss: 0.2573 - val_accuracy: 0.9115 - val_loss: 0.2445\n",
      "Epoch 13/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9164 - loss: 0.2296 - val_accuracy: 0.9053 - val_loss: 0.2020\n",
      "Epoch 14/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9277 - loss: 0.2132 - val_accuracy: 0.9346 - val_loss: 0.2131\n",
      "Epoch 15/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9298 - loss: 0.1890 - val_accuracy: 0.9269 - val_loss: 0.2252\n",
      "Epoch 16/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9287 - loss: 0.1965 - val_accuracy: 0.8902 - val_loss: 0.2035\n",
      "Epoch 17/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9276 - loss: 0.1895 - val_accuracy: 0.9269 - val_loss: 0.1920\n",
      "Epoch 18/100\n",
      "\u001b[1m200/200\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9443 - loss: 0.1657 - val_accuracy: 0.8500 - val_loss: 0.3424\n",
      "\u001b[1m12/12\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8764 - loss: 0.2829\n",
      "Test accuracy: 0.9107142686843872\n",
      "Test loss: 0.17638857662677765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\imani\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    }
   ],
   "source": [
    "model = train_model(build_model())\n",
    "\n",
    "loss, accuracy = model.evaluate(validation_dataset)\n",
    "print(f\"Test accuracy: {accuracy}\")\n",
    "print(f\"Test loss: {loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved successfully!\n"
     ]
    }
   ],
   "source": [
    "choice = input(\"Do you want to save the model? (y/n): \")\n",
    "\n",
    "if choice.lower() == 'y':\n",
    "    model.save(f\"../models/model_acc_{accuracy:.2f}_loss_{loss:.2f}.h5\")\n",
    "    print(\"Model saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gesture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
